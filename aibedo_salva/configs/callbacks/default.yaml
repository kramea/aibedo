# ----------- A set of helpful default callbacks -------------
# NOTE: `monitor` and `mode` are defined in configs/model/base_model_config.yaml


# The following will save the k best model weights to <ckpt_dir>/<filename_k>.pt
# 'best' is defined by the value of the `monitor` metric (usually validation MSE).
model_checkpoint:
  _target_: pytorch_lightning.callbacks.ModelCheckpoint
  monitor: ${model.monitor}   # name of the logged metric which determines when model is improving
  mode: ${model.mode}         # "max" means higher metric value is better, can be also "min"
  save_top_k: 1               # save k best models (determined by above metric)
  save_last: True             # additionally always save model from last epoch
  verbose: ${verbose}
  dirpath: ${ckpt_dir}
  filename: "epoch{epoch:03d}_seed${seed}"
  auto_insert_metric_name: False
  # every_n_epochs: 10

# Early stopping based on the `monitor` metric (usually validation MSE).
early_stopping:
  _target_: pytorch_lightning.callbacks.EarlyStopping
  monitor: ${model.monitor}  # name of the logged metric which determines when model is improving
  mode: ${model.mode}        # "min" means higher metric value is better, can be also "max"
  patience: 5                # how many validation epochs of not improving until training stops
  min_delta: 0               # minimum change in the monitored metric needed to qualify as an improvement

# This will log the LR as function of the #steps/epochs
learning_rate_logging:
 _target_: pytorch_lightning.callbacks.LearningRateMonitor

# Pretty printing of the model architecture, requires rich package
model_summary:
  _target_: pytorch_lightning.callbacks.RichModelSummary
  max_depth: 1

rich_progress_bar:
  _target_: pytorch_lightning.callbacks.RichProgressBar
